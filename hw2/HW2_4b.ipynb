{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import os\n",
    "import pickle\n",
    "from load_dc import load_preprocessed_training_batch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load training, validation, testing set from your preprocessed files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data is loaded from load_dc\n",
    "# which contains functions similar to load_cifar\n",
    "# this is done in the tensorflow session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "net_features = tf.placeholder(tf.float32, shape=(None, 227, 227, 3))\n",
    "net_labels = tf.placeholder(tf.float32, shape=(None, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AlexNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alexnet consists of 5 convolutional layers \n",
    "# the first two are following by overlapping max pool\n",
    "# the last 3 are not, then 2 fully connected\n",
    "# relu is used as the activation\n",
    "\n",
    "conv_1 = tf.layers.conv2d(net_features, filters=96\n",
    "                          ,kernel_size=11, strides=4,\n",
    "                         activation=tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling2d(conv_1, pool_size=3, strides=2)\n",
    "padding_1 = tf.constant([[0,0], [2,2], [2,2], [0,0]])\n",
    "out_1 = tf.pad(max_pool_1, padding_1)\n",
    "conv_2 = tf.layers.conv2d(out_1, filters=256,\n",
    "                          kernel_size=5, strides=1, \n",
    "                          activation=tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling2d(conv_2, pool_size=3,\n",
    "                                    strides=2)\n",
    "# one pad, can reuse\n",
    "padding_2 = tf.constant([[0,0], [1,1], [1,1], [0,0]])\n",
    "out_2 = tf.pad(max_pool_2, padding_2)\n",
    "conv_3 = tf.layers.conv2d(out_2, filters=384, kernel_size=3, \n",
    "                          strides=1, activation=tf.nn.relu)\n",
    "out_3 = tf.pad(conv_3, padding_2)\n",
    "conv_4 = tf.layers.conv2d(out_3, filters=384, \n",
    "                          kernel_size=3, strides=1, \n",
    "                          activation=tf.nn.relu)\n",
    "out_4 = tf.pad(conv_4, padding_2)\n",
    "conv_5 = tf.layers.conv2d(out_4, filters=256, kernel_size=3, \n",
    "                          strides=1,activation=tf.nn.relu)\n",
    "max_pool_3 = tf.layers.max_pooling2d(conv_5, pool_size=3, strides=2)\n",
    "# not really a conv, but map to fc\n",
    "conv_6 = tf.layers.conv2d(max_pool_3, filters=4096, \n",
    "                          kernel_size=6, strides=1, \n",
    "                          activation=tf.nn.relu)\n",
    "# flatten to fully connected\n",
    "out_6 = tf.layers.flatten(conv_6)\n",
    "fc_1 = tf.layers.dense(out_6, units=4096, activation=tf.nn.relu)\n",
    "fc_2 = tf.layers.dense(fc_1, units=4096, activation=tf.nn.relu)\n",
    "output = tf.layers.dense(fc_2, units=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=net_labels, logits=output))\n",
    "opt = tf.train.AdamOptimizer()\n",
    "to_minimize = opt.minimize(cost)\n",
    "\n",
    "# compare prediction accuracy \n",
    "correct_pred = tf.equal(tf.argmax(tf.nn.softmax(output),1),tf.argmax(net_labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and validation</h1>\n",
    "<h2>Train your model only 10 epochs</h2>\n",
    "<p style=\"font-size:20px\">1. Print out training accuracy and validation accuracy each training epoch</p>\n",
    "<p style=\"font-size:20px\">2. Print out training time each training epoch</p>\n",
    "<p style=\"font-size:20px\">3. Your goal is to reach 85% validation accuracy in 10 training epochs. If you reach that, you can perform testing, print out your test accuracy. Plot out the ten images with title that contains the probability of the labeled class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Epoch: \t0\n",
      "Training for Batch: \t0\n"
     ]
    }
   ],
   "source": [
    "# copy and pasted from previous problem and modified as needed\n",
    "start = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        print('Training for Epoch: \\t' + str(i))\n",
    "        batch_num = 0\n",
    "        while batch_num < 5:\n",
    "            print('Training for Batch: \\t' + str(batch_num))\n",
    "            training_data = load_preprocessed_training_batch(batch_num, batch_size)\n",
    "            for data, labels in training_data:\n",
    "                f_dict = {net_features: data, net_labels: labels}\n",
    "                sess.run(to_minimize, feed_dict=f_dict)\n",
    "            batch_num += 1\n",
    "        #if i % 5 == 0:\n",
    "        print('Current Loss is : \\t')\n",
    "        current_loss = sess.run(cost, feed_dict=f_dict)\n",
    "        print(current_loss)\n",
    "        print('Current minibatch accuracy is: \\t')\n",
    "        current_acc = sess.run(accuracy, feed_dict=f_dict)\n",
    "        print(current_acc)\n",
    "        #print('Current validation set accuracy is: \\t')\n",
    "        #val_acc = sess.run(accuracy, feed_dict={x_data:val_data, y_labels:val_labels})\n",
    "        print(val_acc)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
