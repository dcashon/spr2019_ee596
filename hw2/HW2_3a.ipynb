{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T16:27:14.258592Z",
     "start_time": "2019-04-30T16:27:13.242243Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "from sklearn.utils import shuffle\n",
    "#import matplotlib.pyplot as plt\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extract MNIST data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T16:21:56.453161Z",
     "start_time": "2019-04-30T16:21:46.337629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding, reshape = False (that means images are not flatten)\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",reshape=False,one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare training, validation and testing data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:16.017081Z",
     "start_time": "2019-04-30T17:05:14.782724Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "x_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "x_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "#pad images with 0s (28x28 to 32x32)\n",
    "x_train_pad = np.pad(x_train, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0)\n",
    "x_validation_pad = np.pad(x_validation, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0)\n",
    "x_test_pad = np.pad(x_test, ((0,0), (2,2), (2,2), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "# will use a generator function...\n",
    "def get_batches(datax, datay, batch_size):\n",
    "    idx = 0\n",
    "    # need to shuffle here, but its chill\n",
    "    while idx * batch_size < datax.shape[0]:\n",
    "        start = idx * batch_size\n",
    "        end = idx * batch_size + batch_size\n",
    "        yield (datax[start:end], datay[start:end])\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:16.032705Z",
     "start_time": "2019-04-30T17:05:16.017081Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size, num_epochs = 128, 30\n",
    "\n",
    "training_batches = get_batches(x_train_pad, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T16:58:59.356544Z",
     "start_time": "2019-04-30T16:58:59.325654Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:16.048331Z",
     "start_time": "2019-04-30T17:05:16.032705Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:16.908006Z",
     "start_time": "2019-04-30T17:05:16.876499Z"
    }
   },
   "outputs": [],
   "source": [
    "x_in = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:17.827214Z",
     "start_time": "2019-04-30T17:05:17.686347Z"
    }
   },
   "outputs": [],
   "source": [
    "# LeNet 5 implements trainable average pooling layers\n",
    "# in addition to convolutional layers that dont convolve over the\n",
    "# whole feature map. Here we omit these features and\n",
    "# copy the general structure, using max pool, and full conv layers\n",
    "# They also use sigmoids in the convolution, omitted here\n",
    "# and RBF for loss. we just use cross entropy\n",
    "\n",
    "# c1: convolution2d\n",
    "c1 = tf.layers.conv2d(x_in, filters=6, kernel_size=5, activation=tf.nn.sigmoid)\n",
    "\n",
    "# s1: max pool substituted for trainable average pool\n",
    "s1 = tf.layers.max_pooling2d(c1, pool_size=2, strides=2)\n",
    "\n",
    "# c2: convolution 2d size (5,5, 10)\n",
    "c2 = tf.layers.conv2d(s1, filters=16, kernel_size=5, activation=tf.nn.sigmoid)\n",
    "\n",
    "# s3: max pool substitute for ave pool\n",
    "s2 = tf.layers.max_pooling2d(c2, pool_size=2, strides=2)\n",
    "\n",
    "# c4: 1x1 conv layer\n",
    "c3 = tf.layers.conv2d(s2, filters=120, kernel_size=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "# flatten for fully connect\n",
    "c3_flat = tf.layers.flatten(c3)\n",
    "\n",
    "# dense\n",
    "d4 = tf.layers.dense(c3_flat, units=84, activation=tf.nn.sigmoid)\n",
    "\n",
    "# output\n",
    "out = tf.layers.dense(d4, units=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:05:18.967603Z",
     "start_time": "2019-04-30T17:05:18.686345Z"
    }
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out))\n",
    "opt = tf.train.AdamOptimizer()\n",
    "to_minimize = opt.minimize(cost)\n",
    "\n",
    "# compare prediction accuracy \n",
    "correct_pred = tf.equal(tf.argmax(tf.nn.softmax(out),1),tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validating, testing</h1>\n",
    "<h2>1. Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2. Print out training time on each epoch</h2>\n",
    "<h2>3. Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:07:14.956886Z",
     "start_time": "2019-04-30T17:05:19.623843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Epoch \t0\n",
      "0.10227273\n",
      "Val Acc\n",
      "0.1126\n",
      "Training on Epoch \t1\n",
      "0.10227273\n",
      "Val Acc\n",
      "0.1126\n",
      "Training on Epoch \t2\n",
      "0.79545456\n",
      "Val Acc\n",
      "0.7476\n",
      "Training on Epoch \t3\n",
      "0.9431818\n",
      "Val Acc\n",
      "0.915\n",
      "Training on Epoch \t4\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.9386\n",
      "Training on Epoch \t5\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.9522\n",
      "Training on Epoch \t6\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.9604\n",
      "Training on Epoch \t7\n",
      "0.96590906\n",
      "Val Acc\n",
      "0.9652\n",
      "Training on Epoch \t8\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.969\n",
      "Training on Epoch \t9\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.973\n",
      "Training on Epoch \t10\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.9752\n",
      "Training on Epoch \t11\n",
      "0.97727275\n",
      "Val Acc\n",
      "0.977\n",
      "Training on Epoch \t12\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.979\n",
      "Training on Epoch \t13\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9804\n",
      "Training on Epoch \t14\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9806\n",
      "Training on Epoch \t15\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9808\n",
      "Training on Epoch \t16\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9822\n",
      "Training on Epoch \t17\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9822\n",
      "Training on Epoch \t18\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9828\n",
      "Training on Epoch \t19\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9826\n",
      "Training on Epoch \t20\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9824\n",
      "Training on Epoch \t21\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.983\n",
      "Training on Epoch \t22\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9832\n",
      "Training on Epoch \t23\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9834\n",
      "Training on Epoch \t24\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9832\n",
      "Training on Epoch \t25\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.983\n",
      "Training on Epoch \t26\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9826\n",
      "Training on Epoch \t27\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9828\n",
      "Training on Epoch \t28\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9824\n",
      "Training on Epoch \t29\n",
      "0.9886364\n",
      "Val Acc\n",
      "0.9824\n",
      "0.9807\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Training on Epoch \\t\" + str(i))\n",
    "        training_batches = get_batches(x_train_pad, y_train, batch_size)\n",
    "        for x_data, y_labels in training_batches:\n",
    "            f_dict = {x_in: x_data, y_true: y_labels}\n",
    "            sess.run(to_minimize, feed_dict=f_dict)\n",
    "        current_acc = sess.run(accuracy, feed_dict=f_dict)\n",
    "        print(current_acc)\n",
    "        val_acc = sess.run(accuracy, feed_dict={x_in:x_validation_pad, y_true:y_validation})\n",
    "        print('Val Acc')\n",
    "        print(val_acc)\n",
    "    test_acc = sess.run(accuracy, feed_dict={x_in:x_test_pad, y_true:y_test})\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
