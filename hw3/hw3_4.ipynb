{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from text_utils import TextLoader\n",
    "from tensorflow.contrib import rnn\n",
    "from char_rnn_model import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data locations, etc\n",
    "data_dir = '.'\n",
    "\n",
    "# training\n",
    "num_epochs = 5\n",
    "batch_size = 512\n",
    "\n",
    "# network architecture\n",
    "num_neurons = 256\n",
    "seq_length = 50\n",
    "vocab_size = 67\n",
    "num_layers = 2 \n",
    "dropout = False\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data using TextLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('.', 20, 50)\n",
    "#loader.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char_model = Model(num_neurons, seq_length, vocab_size, num_layers=3, lr=0.001, use_dropout=True, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Training is done via slurm job on Hyak P100\n",
    "## Results can be examined in:\n",
    "## slurm-869469.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Epoch Number: \t0\n",
      "Loading Data...\n",
      "Batching Data on:\t 0\n",
      "Beginning Training:\n",
      "(512, 50, 67)\n",
      "4.199327\n",
      "(512, 50, 67)\n",
      "4.1913157\n",
      "(512, 50, 67)\n",
      "4.1800666\n",
      "(512, 50, 67)\n",
      "4.155279\n",
      "(512, 50, 67)\n",
      "4.1134667\n",
      "(512, 50, 67)\n",
      "4.0036426\n",
      "(512, 50, 67)\n",
      "3.7244954\n",
      "(512, 50, 67)\n",
      "3.6742742\n",
      "(512, 50, 67)\n",
      "3.5836692\n",
      "(512, 50, 67)\n",
      "3.4404502\n",
      "(512, 50, 67)\n",
      "3.5961833\n",
      "(512, 50, 67)\n",
      "3.383543\n",
      "(512, 50, 67)\n",
      "3.5070994\n",
      "(512, 50, 67)\n",
      "3.4058828\n",
      "(512, 50, 67)\n",
      "3.4388597\n",
      "(512, 50, 67)\n",
      "3.3758035\n",
      "(512, 50, 67)\n",
      "3.389394\n",
      "(512, 50, 67)\n",
      "3.3973327\n",
      "(512, 50, 67)\n",
      "3.3220975\n",
      "(512, 50, 67)\n",
      "3.5589917\n",
      "(512, 50, 67)\n",
      "3.4527612\n",
      "(512, 50, 67)\n",
      "3.4907775\n",
      "(512, 50, 67)\n",
      "3.641305\n",
      "(512, 50, 67)\n",
      "3.3651736\n",
      "(512, 50, 67)\n",
      "3.3839407\n",
      "(512, 50, 67)\n",
      "3.2883797\n",
      "(512, 50, 67)\n",
      "3.3301847\n",
      "(512, 50, 67)\n",
      "3.3564317\n",
      "(512, 50, 67)\n",
      "3.3342998\n",
      "(512, 50, 67)\n",
      "3.1910374\n",
      "(512, 50, 67)\n",
      "3.3063033\n",
      "(512, 50, 67)\n",
      "3.398341\n",
      "(512, 50, 67)\n",
      "3.5531979\n",
      "(512, 50, 67)\n",
      "3.4118197\n",
      "(512, 50, 67)\n",
      "3.4141133\n",
      "(512, 50, 67)\n",
      "3.4988472\n",
      "(512, 50, 67)\n",
      "3.4527202\n",
      "(512, 50, 67)\n",
      "3.3094177\n",
      "(512, 50, 67)\n",
      "3.489904\n",
      "(512, 50, 67)\n",
      "3.4030285\n",
      "(512, 50, 67)\n",
      "3.3772614\n",
      "(512, 50, 67)\n",
      "3.286912\n",
      "(512, 50, 67)\n",
      "3.2670155\n",
      "(512, 50, 67)\n",
      "3.3553717\n",
      "(512, 50, 67)\n",
      "3.357782\n",
      "(512, 50, 67)\n",
      "3.2872505\n",
      "(512, 50, 67)\n",
      "3.37821\n",
      "(512, 50, 67)\n",
      "3.310715\n",
      "(512, 50, 67)\n",
      "3.3726156\n",
      "(512, 50, 67)\n",
      "3.393684\n",
      "(512, 50, 67)\n",
      "3.3125534\n",
      "(512, 50, 67)\n",
      "3.386759\n",
      "(512, 50, 67)\n",
      "3.2574844\n",
      "(512, 50, 67)\n",
      "3.2737894\n",
      "(512, 50, 67)\n",
      "3.2578254\n",
      "(512, 50, 67)\n",
      "3.293218\n",
      "(512, 50, 67)\n",
      "3.3740659\n",
      "(512, 50, 67)\n",
      "3.2292187\n",
      "(512, 50, 67)\n",
      "3.2649364\n",
      "(512, 50, 67)\n",
      "3.234713\n",
      "(512, 50, 67)\n",
      "3.2042937\n",
      "(512, 50, 67)\n",
      "3.2473722\n",
      "(512, 50, 67)\n",
      "3.2069523\n",
      "(512, 50, 67)\n",
      "3.3487775\n",
      "(512, 50, 67)\n",
      "3.411574\n",
      "(512, 50, 67)\n",
      "3.4193597\n",
      "(512, 50, 67)\n",
      "3.1999135\n",
      "(512, 50, 67)\n",
      "3.2316055\n",
      "(512, 50, 67)\n",
      "3.3865619\n",
      "(512, 50, 67)\n",
      "3.4005294\n",
      "(512, 50, 67)\n",
      "3.2616236\n",
      "(512, 50, 67)\n",
      "3.281631\n",
      "(512, 50, 67)\n",
      "3.195199\n",
      "(512, 50, 67)\n",
      "3.1632552\n",
      "(512, 50, 67)\n",
      "3.1792624\n",
      "(512, 50, 67)\n",
      "3.1879349\n",
      "(512, 50, 67)\n",
      "3.152621\n",
      "(512, 50, 67)\n",
      "3.434653\n",
      "(512, 50, 67)\n",
      "3.2061305\n",
      "(512, 50, 67)\n",
      "3.370939\n",
      "(512, 50, 67)\n",
      "3.268668\n",
      "(512, 50, 67)\n",
      "3.2593584\n",
      "(512, 50, 67)\n",
      "3.2628567\n",
      "(512, 50, 67)\n",
      "3.4442158\n",
      "(512, 50, 67)\n",
      "3.2835865\n",
      "(512, 50, 67)\n",
      "3.3198354\n",
      "(512, 50, 67)\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "i = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epochs in range(num_epochs):\n",
    "        print(\"Training for Epoch Number: \\t\" + str(epochs))\n",
    "        for num in range(5):\n",
    "            print(\"Loading Data...\")\n",
    "            x_train = loader.load_train_data(num)\n",
    "            print(\"Batching Data on:\\t \" + str(num))\n",
    "            batcher = loader.training_batch(batch_size)\n",
    "            print(\"Beginning Training:\")\n",
    "            for data, labels in batcher:\n",
    "                print(data.shape)\n",
    "                i+=1\n",
    "                f_dict = {char_model.X:data[:-1], char_model.Y:data[1:]}\n",
    "                sess.run(char_model.trainer, feed_dict=f_dict)\n",
    "                #test = sess.run(char_model.outputs, feed_dict=f_dict)\n",
    "                #print(test.shape)\n",
    "                if i % 1 ==0:\n",
    "                    c_loss = sess.run(char_model.loss, feed_dict=f_dict)\n",
    "                    print(c_loss)\n",
    "        save_path = saver.save(sess, \"./tmp/model\" + str(epochs) + str(num+15) + \".ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in some val data to test restore\n",
    "x_val = np.load('s_val.npy')\n",
    "# load in the character mapper dictionary\n",
    "vocab = loader.load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions - could have been defined in class\n",
    "def one_hot_2_string(input_onehot, vocab_dict):\n",
    "    \"\"\"\n",
    "    quick function to process output of rnn and map to characters\n",
    "    \n",
    "    \"\"\"\n",
    "    s = \"\"\n",
    "    for row in np.reshape(input_onehot, (50, 67)):\n",
    "        for key, val in vocab_dict.items():\n",
    "            if np.argmax(row) == val:\n",
    "                s += key\n",
    "    return s\n",
    "\n",
    "def string_2_one_hot(input_string, vocab_dict):\n",
    "    \"\"\"\n",
    "    quick function to change string to one-hot encode via vocab dictionary\n",
    "    \"\"\"\n",
    "    encode = []\n",
    "    for char in input_string:\n",
    "        encode.append(np.eye(len(vocab))[vocab_dict[char]])\n",
    "    encode_np = np.array(encode)\n",
    "    x,y = encode_np.shape\n",
    "    return np.reshape(encode_np, (1, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model_0244.ckpt\n",
      "New Generated Shakespeare Text!!! \n",
      "\n",
      "First Citizen:  Before we proceed any further, hear me,\n",
      "And therefore they are all the state of the state,\n",
      "And shall be so far as I am so south and search to thee and stand\n",
      "The state of the sea and the start of the sea,\n",
      "And then the strong and sound of the seas and straight,\n",
      "And therefore shall I see the country to the state.\n",
      "\n",
      "CADE:\n",
      "Ay, and the commons shall be so many\n",
      "the star and the stars of the state of the soldiers,\n",
      "And that the state of the shall be a soldier,\n",
      "And then the seal of the state of the story\n",
      "I have a son and the streets of the\n"
     ]
    }
   ],
   "source": [
    "# here we restore the model, and generate text\n",
    "\n",
    "# get some text to feed in\n",
    "starter = \"First Citizen:  Before we proceed any further, hea\"\n",
    "\n",
    "# map to one-hot\n",
    "encoded_starter = string_2_one_hot(starter, vocab)\n",
    "num_char = 500 # modify to generate more text\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    # restore the session\n",
    "    saver = tf.train.import_meta_graph('./tmp/model_0244.ckpt.meta')\n",
    "    saver.restore(sess, './tmp/model_0244.ckpt')\n",
    "    for num_char in range(num_char):\n",
    "        # feed in the last 50 chars\n",
    "        outputs = sess.run(char_model.logits, feed_dict={char_model.X: string_2_one_hot(starter[-50:], vocab)})\n",
    "        output_encode = one_hot_2_string(outputs, vocab)\n",
    "        # append next prediction to output\n",
    "        starter += output_encode[-1] # last character of predicted output\n",
    "    print('New Generated Shakespeare Text!!! \\n')\n",
    "    print(starter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text ^^ see above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
